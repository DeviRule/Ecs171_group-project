{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Sample: train.csv\n",
    "#### Evaluation Sample: validation_under.csv\n",
    "#### Method: OOB\n",
    "#### Output: Best hyperparameters; Pr-curve; ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer,classification_report, matthews_corrcoef, accuracy_score, average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data is read and named as the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('train.csv')\n",
    "X_train = transactions.drop(labels='Class', axis=1)\n",
    "y_train = transactions.loc[:,'Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 0\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=1)\n",
    "\n",
    "if test== 0:\n",
    "    n_estimators = [75,150,800,1000,1200]\n",
    "    min_samples_split = [2, 5]\n",
    "    min_samples_leaf = [1, 5]\n",
    "else:\n",
    "    n_estimators = [70]\n",
    "    min_samples_split = [2]\n",
    "    min_samples_leaf = [1]\n",
    "\n",
    "param_grid_rf = {'n_estimators': n_estimators,\n",
    "                 'min_samples_split': min_samples_split,\n",
    "                 'min_samples_leaf': min_samples_split,\n",
    "                 'oob_score': [True]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf,cv = 5,\n",
    "                       n_jobs=-1, pre_dispatch='2*n_jobs', verbose=1, return_train_score=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best score and the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.read_csv('validation_under.csv')\n",
    "X_eval = evaluation.drop(labels='Class', axis=1)\n",
    "y_eval = evaluation.loc[:,'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_eval(estimator, X_test, y_test):\n",
    "    \n",
    "    y_pred = estimator.predict(X_test)\n",
    "\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    if y_test.nunique() <= 2:\n",
    "        try:\n",
    "            y_score = estimator.predict_proba(X_test)[:,1]\n",
    "        except:\n",
    "            y_score = estimator.decision_function(X_test)\n",
    "        print('AUPRC', average_precision_score(y_test, y_score))\n",
    "        print('AUROC', roc_auc_score(y_test, y_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest_eval(grid_rf, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision_rf,recall_rf,thresholds_rf = precision_recall_curve(\n",
    "    y_eval, grid_rf.predict_proba(X_eval)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf * 0.5))\n",
    "plt.plot(precision_rf,recall_rf,label=\"rf\")\n",
    "plt.plot(precision_rf[close_default_rf],recall_rf[close_default_rf],'^',c='k',\n",
    "        label = \"threashold 0.5 rf\")\n",
    "plt.xlabel(\"precision\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"PR_Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Receiver Operating Characteristic Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_rf,tpr_rf,th_rf = roc_curve(\n",
    "    y_eval, grid_rf.predict_proba(X_eval)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_rf = np.argmin(np.abs(th_rf * 0.5))\n",
    "plt.plot(fpr_rf,tpr_rf,label=\"rf\")\n",
    "plt.plot(fpr_rf[close_default_rf],tpr_rf[close_default_rf],'^',c='k',\n",
    "        label = \"threashold 0.5 rf\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"ROC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
